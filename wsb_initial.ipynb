{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5b65f50",
   "metadata": {},
   "source": [
    "## Problem Statement and Initial Ideas\n",
    "\n",
    "**Problem Statement:**\n",
    "\n",
    "The wallstreetbets reddit community gets a lot of flack for its incredibly bold -- at times, downright absurd -- trades. However, it was very right about the GME saga, thanks for the EXPR adn AMC sympathy gains. How can I measure the success of the community in taking certain trade positions? \n",
    "\n",
    "**Initial Idea:** \n",
    "\n",
    "I would need to consider what stocks to look for and where to look for them within wallstreetbets. Would it be a good idea to look for all traded stocks on the NYSE? Certain industries? The NASDAQ (ultimately came back to this one)? \n",
    "\n",
    "I began by trying to consider the stocks in the S&P 500. Webscraping these stocks from wikipedia was simple enough and shown below. The initial idea was to look at the section of posts called 'due dilligence.' Here, redditors post their trade ideas and justifications, sometimes justification is a stretch. \n",
    "\n",
    "The idea would be to scrape the post titles, then see the number of likes, comments, and the stocks mentioned. If I could get a substantial list of posts, then I could use the number of likes and comments as a benchmark for how much redditors liked the due diligence and, subsequently, the stock. \n",
    "\n",
    "However, this idea proved flawed. It would be difficult to get enough variation of stocks with this method. The post titles often did not clearly state the stock by ticker (ex: AAPL or $APPL). The spirit of the idea did prove helpful though: mentions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997fe8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# import s and p 500 companies only \n",
    "url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'\n",
    "headers = { 'User-Agent':'Mozilla/5.0'}\n",
    "\n",
    "page = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "# clean way to get data table with all info\n",
    "stocktable = soup.find('table', {'class': 'wikitable'}) # gets first instance\n",
    "df = pd.read_html(str(stocktable))\n",
    "df=pd.DataFrame(df[0])\n",
    "\n",
    "\n",
    "# dirty way to get stock symbols as list \n",
    "tab = soup.find('table', class_= \"wikitable\")\n",
    "test = [stock.text for stock in tab.find_all('a', rel = 'nofollow')]\n",
    "ticker = [stock for stock in test if stock != 'reports']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018feeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLRY needs your help!\n",
      "Remember January 2021? Pepperidge Farm Remembers. What's more, it looks like things are stacking up for not only a repeat, but a larger run.\n",
      "The VCs are all making money on Heliogen (HLGN) and it's time we did too (by shorting it)\n",
      "Undervalued Stock $GS\n",
      "Listen up dingleberries, you're about to miss the ‚ò¢Ô∏èuranium‚ò¢Ô∏è rocketshipüöÄüöÄüöÄ\n",
      "$LCID puts are free money\n",
      "$LMND clearly a short target Come on Apes break shorty\n",
      "SKLZ at the all time low. Upside inevitable?\n",
      "Due diligence: AAPL and TSLA\n",
      "$MYBUF | $BORNY The Most Significant Advancement in Science Since They Invented the Sun DD\n",
      "$ALL state fueling up the rocket. That's Allstate's stand.\n",
      "$LH My fellow retards is what we have been waiting for‚Ä¶ The stars align\n",
      "Should you listen to Jim Cramer? - I analyzed 20,000+ recommendations made by Jim Cramer during the last 5 years. Here are the results.\n",
      "$ZIP: set up for a massive run in 2022\n",
      "DD - What are Dark Pools and how do they work?\n",
      "MSGS - Madison Square Garden significantly undervalued\n",
      "Healthcare/Biotech Sector DD (what to look for and how to value companies)\n",
      "DD - DISCOVERY: DISCK Don't Fail Me Now - Balls Deep Value for 2022\n",
      "Why $OSTK is undervalued and I‚Äôm tempted to go all inüöÄ\n",
      "BioTech Investors Beware\n",
      "$PL Planet Labs PBC trading at a triple bottom\n",
      "AutoNation - Due Diligence\n",
      "Why Dry Bulk Shipping is going to be a big winner in 2022/2023. The sector is entering a multi-year super cycle.\n",
      "Behold the 3rd largest automaker!\n",
      "Cash in on Pelosi and the CHIPS and FABS Acts\n"
     ]
    }
   ],
   "source": [
    "# when idea was to look at the posts for due diligence and see the mentions of stocks\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "\n",
    "url = 'https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ADD'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "page = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "for post in soup.find_all('a', attrs={'class': 'search-title'}):\n",
    "    print(post.text)\n",
    "    \n",
    "posts = [post.text for post in soup.find_all('a', attrs={'class': 'search-title'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bd380e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the past months due diligence \n",
    "\n",
    "days_ago = 0\n",
    "url = 'https://ns.reddit.com/r/wallstreetbets/search?sort=new&restrict_sr=on&q=flair%3ADD'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "page = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "while days_ago <=31:\n",
    "\n",
    "    for post in soup.find_all('div', class_ = \"search-result\"):\n",
    "\n",
    "        # get post name, likes, comments, and days ago submitted \n",
    "        name = post.find('a', class_ = 'search-title').text\n",
    "        # issue is new posts may not have likes, comments, or even content\n",
    "        if post.find('div', class_ = 'md') is None: \n",
    "            content = \"\"\n",
    "        else: \n",
    "            content = post.find('div', class_ = 'md').text\n",
    "        comments = post.find('a', class_='search-comments').text.split(' ')[0]\n",
    "        if post.find('span', class_ = 'search-score') is None: \n",
    "            likes = 0\n",
    "        else: \n",
    "            likes = post.find('span', class_ = 'search-score').text.split(' ')[0]\n",
    "        days_ago = post.find('span', class_ = 'search-time').text.split(' ')[1:]\n",
    "        if days_ago[1] == 'hours':\n",
    "            days_ago = 0\n",
    "        elif days_ago[1] == 'day':\n",
    "            days_ago = 1\n",
    "        elif days_ago[1] == 'days':\n",
    "            days_ago = int(days_ago[0])\n",
    "        elif days_ago[1] == 'month':\n",
    "            days_ago = 32\n",
    "            \n",
    "        result = [name, content, likes, comments, days_ago]\n",
    "\n",
    "        with open('dd.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(result)\n",
    "            \n",
    "    next_button = soup.find('span', class_ = 'nextprev')\n",
    "    next_page_link = next_button.find(\"a\", {'rel':'nofollow next'}).attrs['href']\n",
    "    time.sleep(2)\n",
    "    page = requests.get(next_page_link, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ed5bb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>content</th>\n",
       "      <th>likes</th>\n",
       "      <th>comments</th>\n",
       "      <th>days_ago</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remember January 2021? Pepperidge Farm Remembe...</td>\n",
       "      <td>Hi everyone, bob here.\\nI posted this same dat...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>['44', 'minutes', 'ago']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The VCs are all making money on Heliogen (HLGN...</td>\n",
       "      <td>Summary\\nHeliogen (HLGN) got a solid pump on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>['50', 'minutes', 'ago']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Undervalued Stock $GS</td>\n",
       "      <td>I know, I know.  Someone is recommending a ban...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>['51', 'minutes', 'ago']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Listen up dingleberries, you're about to miss ...</td>\n",
       "      <td>Uranium has had a few parabolic-type moves in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>['1', 'hour', 'ago']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$LCID puts are free money</td>\n",
       "      <td>Just wanted to give people a heads up that $LC...</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>NIO: Addressing Near-Term Risks.</td>\n",
       "      <td>Call to Action\\nDrawing attention to the key d...</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>PLBY - Cardi B named \"Creative Director in Res...</td>\n",
       "      <td>Keeping this one short and sweet. I've been sh...</td>\n",
       "      <td>469</td>\n",
       "      <td>260</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>$Qdel, trading at 6 times their q1 2022 earnings?</td>\n",
       "      <td>After their 25.6m test kits (284.2$M)1 Federal...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Ballsack Nike Projections? üëüüí©üëüüí©üöÄüòõüöÄüòõ</td>\n",
       "      <td>Nike no no yummy puts üí©üöÄ\\nGiven the current nu...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>(12/2) Thursday's Pre-Market Stock Movers &amp; News</td>\n",
       "      <td>Good morning traders and investors of the r/wa...</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  post  \\\n",
       "0    Remember January 2021? Pepperidge Farm Remembe...   \n",
       "1    The VCs are all making money on Heliogen (HLGN...   \n",
       "2                                Undervalued Stock $GS   \n",
       "3    Listen up dingleberries, you're about to miss ...   \n",
       "4                            $LCID puts are free money   \n",
       "..                                                 ...   \n",
       "169                   NIO: Addressing Near-Term Risks.   \n",
       "170  PLBY - Cardi B named \"Creative Director in Res...   \n",
       "171  $Qdel, trading at 6 times their q1 2022 earnings?   \n",
       "172                Ballsack Nike Projections? üëüüí©üëüüí©üöÄüòõüöÄüòõ   \n",
       "173   (12/2) Thursday's Pre-Market Stock Movers & News   \n",
       "\n",
       "                                               content likes  comments  \\\n",
       "0    Hi everyone, bob here.\\nI posted this same dat...     0        48   \n",
       "1    Summary\\nHeliogen (HLGN) got a solid pump on t...     0         3   \n",
       "2    I know, I know.  Someone is recommending a ban...     0         9   \n",
       "3    Uranium has had a few parabolic-type moves in ...     0        24   \n",
       "4    Just wanted to give people a heads up that $LC...    39        40   \n",
       "..                                                 ...   ...       ...   \n",
       "169  Call to Action\\nDrawing attention to the key d...     3        34   \n",
       "170  Keeping this one short and sweet. I've been sh...   469       260   \n",
       "171  After their 25.6m test kits (284.2$M)1 Federal...    10        11   \n",
       "172  Nike no no yummy puts üí©üöÄ\\nGiven the current nu...     2        12   \n",
       "173  Good morning traders and investors of the r/wa...    32         6   \n",
       "\n",
       "                     days_ago  \n",
       "0    ['44', 'minutes', 'ago']  \n",
       "1    ['50', 'minutes', 'ago']  \n",
       "2    ['51', 'minutes', 'ago']  \n",
       "3        ['1', 'hour', 'ago']  \n",
       "4                           0  \n",
       "..                        ...  \n",
       "169                        32  \n",
       "170                        32  \n",
       "171                        32  \n",
       "172                        32  \n",
       "173                        32  \n",
       "\n",
       "[174 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data frame of past months due diligence p \n",
    "dd = pd.read_csv('dd.csv', header = 0, names = [\"post\", \"content\", \"likes\", \"comments\", \"days_ago\"])\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831fdf1",
   "metadata": {},
   "source": [
    "## Pivot: Daily Discussion and Most Mentioned\n",
    "\n",
    "It did not seem that the above method would really be able to quantify the amount of support of the WSB community in the same manner as comment mentions within the daily discussion. This discussion is very raw and unfiltered, and the timing of a new one each day really isolates each post to one trading day. So, I forged ahead with getting the most mentioned tickers of the daily discussion. \n",
    "\n",
    "**Step 1**: Get stocks to track mentions in the daily discussion comments. \n",
    "I decided to get the nasdaq stocks by webscraping them off nasdaqtrader.com. Then create a dictionary which will be updated later as post comments are iterated over. \n",
    "\n",
    "**Step 2**: Get the id's for the each daily discussion page to pass along to PRAW, a Python Reddit API Wrapper. \n",
    "These id's were scraped using the beautiful soup library. \n",
    "\n",
    "**Step 3**: From there, I scraped the data for november daily discussion comments. I had to use the parent comments only, meaning replies to the comments were not included. This is justifiable since popular tickers will still be mentioned many more times throughout the day if all children comments (replies) are not scraped. I did this for December 31 2021 as a test. Then, repeated the process for an entire month of data -- november 2020. \n",
    "\n",
    "**Step 4**: The function 'toptickers' takes two arguments -- month and top. It scrapes daily discussion comments from 2021 for the 'month' inputted and returns a list of length 'top' of the most mentioned tickers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "292789c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of nasdaq stocks and val which will be updated when found in wsb reddit comments \n",
    "import pandas as pd \n",
    "\n",
    "url = 'http://www.nasdaqtrader.com/dynamic/symdir/nasdaqlisted.txt'\n",
    "nasdaq = pd.read_csv(url, sep = '|')\n",
    "\n",
    "stocks = list(nasdaq['Symbol'])\n",
    "# create list of 1's length of symbols\n",
    "val = [0] * len(stocks)\n",
    "\n",
    "lst1 = zip(stocks, val)\n",
    "stocks_dict = dict(lst1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cbe85ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use webscraping to get all the needed id's so they can be mad einto list and passed to create praw objects to get comments\n",
    "# maybe also scrape the last three strings to get mon-day-year for post\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "\n",
    "counter = 0 \n",
    "url = 'https://old.reddit.com/r/wallstreetbets/search?q=flair_name%3A%22Daily+Discussion%22&restrict_sr=1&sort=new'\n",
    "headers= {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "page = requests.get(url,headers=headers)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "while counter <= 100:\n",
    "\n",
    "    for post in soup.find_all('div', class_ = \"search-result\"):\n",
    "\n",
    "        # get post name, likes, comments, and days ago submitted \n",
    "        month, day, year = post.find('a', class_ = 'search-title').text.split(' ')[-3:]\n",
    "        ref = post.find('a', {'class': 'search-title'}).attrs['href'].split('/')[6]\n",
    "        result = [month, day, year,ref]\n",
    "        with open('wsb_daily.csv', 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(result)\n",
    "    \n",
    "        counter += 1\n",
    "        \n",
    "    next_button = soup.find('span', class_ = 'nextprev')\n",
    "    next_page_link = next_button.find(\"a\", {'rel':'nofollow next'}).attrs['href']\n",
    "    time.sleep(2)\n",
    "    page = requests.get(next_page_link, headers=headers)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea5c73cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>December</td>\n",
       "      <td>13</td>\n",
       "      <td>2021</td>\n",
       "      <td>rey64p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>December</td>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>rd68ig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>December</td>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "      <td>rcr4sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>December</td>\n",
       "      <td>09</td>\n",
       "      <td>2021</td>\n",
       "      <td>rcfcod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>December</td>\n",
       "      <td>09</td>\n",
       "      <td>2021</td>\n",
       "      <td>rc0ux0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>December</td>\n",
       "      <td>08</td>\n",
       "      <td>2021</td>\n",
       "      <td>rbov18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>December</td>\n",
       "      <td>08</td>\n",
       "      <td>2021</td>\n",
       "      <td>rb9dg9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>December</td>\n",
       "      <td>07</td>\n",
       "      <td>2021</td>\n",
       "      <td>rawn4n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>December</td>\n",
       "      <td>07</td>\n",
       "      <td>2021</td>\n",
       "      <td>rahlge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>December</td>\n",
       "      <td>06</td>\n",
       "      <td>2021</td>\n",
       "      <td>ra4qvi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>December</td>\n",
       "      <td>06</td>\n",
       "      <td>2021</td>\n",
       "      <td>r9p9ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>December</td>\n",
       "      <td>03</td>\n",
       "      <td>2021</td>\n",
       "      <td>r7wo5m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>December</td>\n",
       "      <td>03</td>\n",
       "      <td>2021</td>\n",
       "      <td>r7h6m8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>December</td>\n",
       "      <td>02</td>\n",
       "      <td>2021</td>\n",
       "      <td>r7505w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>December</td>\n",
       "      <td>02</td>\n",
       "      <td>2021</td>\n",
       "      <td>r6p244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>December</td>\n",
       "      <td>01</td>\n",
       "      <td>2021</td>\n",
       "      <td>r6c8u6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>December</td>\n",
       "      <td>01</td>\n",
       "      <td>2021</td>\n",
       "      <td>r5xalp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>November</td>\n",
       "      <td>30</td>\n",
       "      <td>2021</td>\n",
       "      <td>r5kuzn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>November</td>\n",
       "      <td>30</td>\n",
       "      <td>2021</td>\n",
       "      <td>r55j9p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>November</td>\n",
       "      <td>29</td>\n",
       "      <td>2021</td>\n",
       "      <td>r4tfha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>November</td>\n",
       "      <td>29</td>\n",
       "      <td>2021</td>\n",
       "      <td>r4e8p3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>November</td>\n",
       "      <td>26</td>\n",
       "      <td>2021</td>\n",
       "      <td>r2kpzy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2021,</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>Day</td>\n",
       "      <td>r1u53o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>November</td>\n",
       "      <td>25th</td>\n",
       "      <td>2021</td>\n",
       "      <td>r1f2zm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>November</td>\n",
       "      <td>24</td>\n",
       "      <td>2021</td>\n",
       "      <td>r12ms8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>November</td>\n",
       "      <td>24</td>\n",
       "      <td>2021</td>\n",
       "      <td>r0n4xt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>November</td>\n",
       "      <td>23</td>\n",
       "      <td>2021</td>\n",
       "      <td>r0asp7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>November</td>\n",
       "      <td>23</td>\n",
       "      <td>2021</td>\n",
       "      <td>qzvoy4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>November</td>\n",
       "      <td>22</td>\n",
       "      <td>2021</td>\n",
       "      <td>qziyji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>November</td>\n",
       "      <td>22</td>\n",
       "      <td>2021</td>\n",
       "      <td>qz4b1v</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       month     day  year      id\n",
       "30  December      13  2021  rey64p\n",
       "31  December      10  2021  rd68ig\n",
       "32  December      10  2021  rcr4sg\n",
       "33  December      09  2021  rcfcod\n",
       "34  December      09  2021  rc0ux0\n",
       "35  December      08  2021  rbov18\n",
       "36  December      08  2021  rb9dg9\n",
       "37  December      07  2021  rawn4n\n",
       "38  December      07  2021  rahlge\n",
       "39  December      06  2021  ra4qvi\n",
       "40  December      06  2021  r9p9ti\n",
       "41  December      03  2021  r7wo5m\n",
       "42  December      03  2021  r7h6m8\n",
       "43  December      02  2021  r7505w\n",
       "44  December      02  2021  r6p244\n",
       "45  December      01  2021  r6c8u6\n",
       "46  December      01  2021  r5xalp\n",
       "47  November      30  2021  r5kuzn\n",
       "48  November      30  2021  r55j9p\n",
       "49  November      29  2021  r4tfha\n",
       "50  November      29  2021  r4e8p3\n",
       "51  November      26  2021  r2kpzy\n",
       "52     2021,  Turkey   Day  r1u53o\n",
       "53  November    25th  2021  r1f2zm\n",
       "54  November      24  2021  r12ms8\n",
       "55  November      24  2021  r0n4xt\n",
       "56  November      23  2021  r0asp7\n",
       "57  November      23  2021  qzvoy4\n",
       "58  November      22  2021  qziyji\n",
       "59  November      22  2021  qz4b1v"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives file with the id's and date information of daily discussions\n",
    "# needed to pass to comments scraper\n",
    "wsb = pd.read_csv('wsb_daily.csv', names= ['month', 'day', 'year', 'id'])\n",
    "wsb[\"day\"]=wsb[\"day\"].str.replace(',','')\n",
    "id_list = list(wsb['id'])\n",
    "wsb[30:60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6b6d96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webscraping comments from wsb daily discussion (dec 31) --test run\n",
    "\n",
    "# import praw\n",
    "\n",
    "# submission = reddit.submission(id ='rst2f2')\n",
    "\n",
    "# commentslist = []\n",
    "\n",
    "# submission.comments.replace_more(limit=None)\n",
    "# for comment in submission.comments.list():\n",
    "#     commentslist.append(comment.body)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "95d688ab",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NVDA    22\n",
       "AMD     13\n",
       "AAPL    11\n",
       "QQQ     10\n",
       "ON      10\n",
       "AMZN     9\n",
       "SOFI     7\n",
       "HOOD     6\n",
       "GET      6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking number of appearances of ticker for dec 31\n",
    "# import re \n",
    "\n",
    "# mentions_dict = {}\n",
    "\n",
    "# # regex pattern which will be used to find stocks from stoks_dict in comments\n",
    "# # can refine this to capture other such possibilities \n",
    "# pattern = r'\\b([A-Z]+)\\b'\n",
    "\n",
    "\n",
    "# for comment in commentslist: \n",
    "#     for ticker in re.findall(pattern, comment): \n",
    "#         if ticker in stocks_dict:\n",
    "#             if ticker not in mentions_dict: \n",
    "#                 mentions_dict[ticker] = 1\n",
    "#             else:\n",
    "#                 mentions_dict[ticker] += 1\n",
    "\n",
    "# # top 10 most mentioned (trending) nasdaq stock tickers\n",
    "# trending = pd.Series(mentions_dict).sort_values(ascending = False)\n",
    "# trending[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e9b944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write as a function - get month and list of top most mentioned stocks\n",
    "import praw \n",
    "import re\n",
    "\n",
    "def toptickers(month, top):\n",
    "    '''insert month and number of top stocks, return top mentioned stocks'''\n",
    "    nov = wsb[wsb['month'] == month]\n",
    "    ids = list(nov['id'])\n",
    "    mentions_dict = {}\n",
    "    pattern = r'\\b([A-Z]+)\\b'\n",
    "    \n",
    "    # hid personal info \n",
    "    reddit = praw.Reddit(client_id='your_id', client_secret='your_secret', user_agent='your_agent')\n",
    "    commentslist = []\n",
    "\n",
    "    # # loop over all nov days and id's\n",
    "    for day_id in ids:\n",
    "        tmp = []\n",
    "        submission = reddit.submission(id = str(day_id))\n",
    "        tmp.append(submission.title)\n",
    "        # only parent comments\n",
    "        submission.comments.replace_more(limit=0)\n",
    "\n",
    "        for comment in submission.comments.list():\n",
    "            tmp.append(comment.body) \n",
    "\n",
    "        # saves list of lists of all parent comments\n",
    "        commentslist.append(tmp)\n",
    "\n",
    "        # update the mentions dict\n",
    "        for dailycomment in tmp:\n",
    "            for ticker in re.findall(pattern, dailycomment): \n",
    "                if ticker in stocks_dict:\n",
    "                    if ticker not in mentions_dict: \n",
    "                        mentions_dict[ticker] = 1\n",
    "                    else:\n",
    "                        mentions_dict[ticker] += 1\n",
    "\n",
    "    # top 10 most mentioned (trending) nasdaq stock tickers\n",
    "    trending = pd.Series(mentions_dict).sort_values(ascending = False)\n",
    "    return(trending[0:(top+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "toptickers(\"November\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2f8b2",
   "metadata": {},
   "source": [
    "## Assessment \n",
    "\n",
    "**How to Assess:** From here, there had to be a justifiable measure of the assessment and appropriate way to visualize the results of the wallstreebets community. I decided the best measure for comparison would need the historical pricing of QQQ, since it is an ETF which tracks the NASDAQ.\n",
    "\n",
    "My initial idea is to gather the top ticker mentions on wallstreetbets for a particular month (say November) and track the following months return from market open on the 1st to market close at the end of the month (say December 1 open - December 31 close). \n",
    "\n",
    "Thankfully, the yahoo finance library allows users to track historical prices for a ticker of choice. I created a function 'ticker_change' which takes arguments for ticker -- the ticker of interest -- and mont -- the month to compare. \n",
    "\n",
    "Below, we can see the very negative percent change of the top tickers in November over the period from December 1st until December 31st. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01bd0437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01962124305854888"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now get the price change for QQQ (nasdaq etf) from dec 1 to dec 30 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "qqq = yf.Ticker(\"QQQ\")\n",
    "\n",
    "\n",
    "# get historical market data\n",
    "hist = qqq.history(start=\"2021-12-01\", end=\"2022-01-01\")\n",
    "qqq_change = ((hist['Close'][-1] - hist['Open'][0])/ hist['Open'][0]) * 100\n",
    "qqq_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "909a2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_change(ticker, month):\n",
    "    '''insert stock and month to return percent change for month'''\n",
    "    if month == 'January':\n",
    "        start = '2021-01-01'   \n",
    "        end = '2021-02-01'\n",
    "    elif month == 'February':\n",
    "        start= '2021-02-01'\n",
    "        end = '2021-03-01'\n",
    "    elif month == 'March':\n",
    "        start= '2021-03-01'\n",
    "        end = '2021-04-01'\n",
    "    elif month == 'April':\n",
    "        start= '2021-04-01'\n",
    "        end = '2021-05-01'\n",
    "    elif month == 'May':\n",
    "        start= '2021-05-01'\n",
    "        end = '2021-06-01'\n",
    "    elif month == 'June':\n",
    "        start= '2021-06-01'\n",
    "        end = '2021-07-01'\n",
    "    elif month == 'July':\n",
    "        start= '2021-07-01'\n",
    "        end = '2021-08-01'\n",
    "    elif month == 'August':\n",
    "        start= '2021-08-01'\n",
    "        end = '2021-09-01'\n",
    "    elif month == 'September':\n",
    "        start= '2021-09-01'\n",
    "        end = '2021-10-01'\n",
    "    elif month == 'October':\n",
    "        start= '2021-10-01'\n",
    "        end = '2021-11-01'\n",
    "    elif month == 'November':\n",
    "        start= '2021-11-01'\n",
    "        end = '2021-12-01'\n",
    "    elif month == 'December':\n",
    "        start= '2021-12-01'\n",
    "        end = '2022-01-01'\n",
    "        \n",
    "    ticker = yf.Ticker(str(ticker))\n",
    "\n",
    "\n",
    "    # get historical market data\n",
    "    hist = ticker.history(start= start, end= end)\n",
    "    change = ((hist['Close'][-1] - hist['Open'][0])/ hist['Open'][0]) * 100\n",
    "    return(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5b0ccad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stocks</th>\n",
       "      <th>percent change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>-8.953212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>-11.463324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LCID</td>\n",
       "      <td>-29.641276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMD</td>\n",
       "      <td>-10.270002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RIVN</td>\n",
       "      <td>-13.978761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PYPL</td>\n",
       "      <td>0.431378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TLRY</td>\n",
       "      <td>-31.078428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OCGN</td>\n",
       "      <td>-27.892231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SOFI</td>\n",
       "      <td>-9.200552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.024607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PTON</td>\n",
       "      <td>-18.374807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stocks  percent change\n",
       "0    TSLA       -8.953212\n",
       "1    NVDA      -11.463324\n",
       "2    LCID      -29.641276\n",
       "3     AMD      -10.270002\n",
       "4    RIVN      -13.978761\n",
       "5    PYPL        0.431378\n",
       "6    TLRY      -31.078428\n",
       "7    OCGN      -27.892231\n",
       "8    SOFI       -9.200552\n",
       "9    AAPL        6.024607\n",
       "10   PTON      -18.374807"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run toptickers, get list of ticker, run ticker_change, compare to ticker_change(qqq)\n",
    "\n",
    "# change above tops to ust the tickers here\n",
    "top_ten = list(toptickers('November', 10).index)\n",
    "\n",
    "stocks = []\n",
    "change = []\n",
    "\n",
    "for ticker in top_ten: \n",
    "    val = ticker_change(ticker, \"December\")\n",
    "    stocks.append(ticker)\n",
    "    change.append(val)\n",
    "    \n",
    "\n",
    "d = {'stocks': stocks, 'percent change': change}\n",
    "df = pd.DataFrame(data = d)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
